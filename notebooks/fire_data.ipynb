{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing fire risk factors in NYC\n",
    "\n",
    "## Background\n",
    "Using data provided by NYC OpenData, this notebook walks through the steps of analyzing fire-related incidents and some possible contributing factors in New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "# Interactive maps\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and describe data\n",
    "\n",
    "Note: Data was filtered on the NYC OpenData site to only include incident classification groups that were fire-related (Structural and NonStructural Fires) prior to export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connection to azure database \n",
    "# import pandas as pd, pyodbc\n",
    "# server = 'finalprojectdata.database.windows.net'\n",
    "# database = 'v2-project-data'\n",
    "# username = 'finalproject1_pmprybylski'\n",
    "# password = 'firedispatch1!'\n",
    "# driver= '{ODBC Driver 17 for SQL Server}'\n",
    "# # con_string = 'DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password\n",
    "# # con_string = 'DRIVER={SQL Server};SERVER='+ <server> +';DATABASE=' + <database>\n",
    "# # cnxn = pyodbc.connect(con_string)\n",
    "# cnxn = pyodbc.connect(\n",
    "#     'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "#     'SERVER=finalprojectdata.database.windows.net;'\n",
    "#     'PORT=1433;'\n",
    "#     'DATABASE=v2-project-data;'\n",
    "#     'UID=finalproject1_pmprybylski;'\n",
    "#     'PWD=firedispatch1!;'\n",
    "# )\n",
    "# query = \"\"\"\n",
    "# SELECT TOP 3 * FROM cleaned_fire_dispatch_data\n",
    "# \"\"\"\n",
    "# result_port_map = pd.read_sql(query, cnxn)\n",
    "# result_port_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../data/raw/In-Service_Alarm_Box_Locations.csv does not exist: '../data/raw/In-Service_Alarm_Box_Locations.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-07899285b22e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Fire Incident Dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0malarms_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/raw/In-Service_Alarm_Box_Locations.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdispatch_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/raw/Fire_Incident_Dispatch_Data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfire_counts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/raw/Fire_Counts.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../data/raw/In-Service_Alarm_Box_Locations.csv does not exist: '../data/raw/In-Service_Alarm_Box_Locations.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data into Python\n",
    "\n",
    "# Fire Incident Dispatch\n",
    "alarms_df = pd.read_csv('../data/raw/In-Service_Alarm_Box_Locations.csv')\n",
    "dispatch_df = pd.read_csv('../data/raw/Fire_Incident_Dispatch_Data.csv')\n",
    "fire_counts_df = pd.read_csv('../data/raw/Fire_Counts.csv')\n",
    "\n",
    "# NYPD Complaints\n",
    "nypd_df = pd.read_csv('../data/raw/NYPD_Complaint_18-21.csv')\n",
    "\n",
    "# Dept of Buildings/Environ Control Board Violations\n",
    "DOB18_df = pd.read_csv('../data/raw/DOB_ECB_Violations_18.csv')\n",
    "DOB19_df = pd.read_csv('../data/raw/DOB_ECB_Violations_19.csv')\n",
    "DOB20_df = pd.read_csv('../data/raw/DOB_ECB_Violations_20.csv')\n",
    "DOB21_df = pd.read_csv('../data/raw/DOB_ECB_Violations_21.csv')\n",
    "\n",
    "# Housing Maintenance Code Violations\n",
    "codev_df = pd.read_csv('../data/raw/Housing_Maintenance_Code_Violations_18-21.csv')\n",
    "\n",
    "# Orders to repair/vacate\n",
    "vacate_df = pd.read_csv('../data/raw/Order_to_Repair_Vacate_18-21.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire Incident Dispatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join Fire Dispatch files\n",
    "fires_df = pd.merge(left=alarms_df, right=dispatch_df, left_on='LOCATION', right_on='ALARM_BOX_LOCATION')\n",
    "fires_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "fires_df = fires_df[['STARFIRE_INCIDENT_ID',\n",
    "               'INCIDENT_DATETIME',\n",
    "               'ALARM_BOX_BOROUGH',\n",
    "               'BOROBOX',\n",
    "               'ALARM_BOX_LOCATION',\n",
    "               'LATITUDE',\n",
    "               'LONGITUDE',\n",
    "               'INCIDENT_BOROUGH',\n",
    "               'ZIPCODE',\n",
    "               'INCIDENT_CLASSIFICATION',\n",
    "               'INCIDENT_CLASSIFICATION_GROUP',\n",
    "               'DISPATCH_RESPONSE_SECONDS_QY',\n",
    "               'INCIDENT_RESPONSE_SECONDS_QY',\n",
    "               'INCIDENT_TRAVEL_TM_SECONDS_QY',\n",
    "               'ENGINES_ASSIGNED_QUANTITY',\n",
    "               'LADDERS_ASSIGNED_QUANTITY',\n",
    "               'OTHER_UNITS_ASSIGNED_QUANTITY',]]\n",
    "fires_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data to csv for visualization use\n",
    "fires_df.to_csv('../data/processed/cleaned_fire_dispatch_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fire_counts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert INCIDENT_DATETIME column to datetime\n",
    "fire_counts_df['INCIDENT_DATETIME'] = fire_counts_df['INCIDENT_DATETIME'].apply(lambda x: dt.datetime.strptime(x,'%m/%d/%Y %I:%M:%S %p'))\n",
    "fire_counts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column that splits off the year\n",
    "fire_counts_df['YEAR'] = fire_counts_df['INCIDENT_DATETIME'].dt.year\n",
    "\n",
    "# Move that column to the beginning of the frame\n",
    "year = fire_counts_df['YEAR']\n",
    "fire_counts_df.drop(labels=['YEAR'], axis=1, inplace=True)\n",
    "fire_counts_df.insert(0,'YEAR', year)\n",
    "fire_counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training(18-19) and cross-validation(20-21) dataframes\n",
    "fires1 = fire_counts_df.loc[(fire_counts_df.YEAR == 2018)|(fire_counts_df.YEAR == 2019)]\n",
    "fires1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires2 = fire_counts_df.loc[(fire_counts_df.YEAR == 2020)|(fire_counts_df.YEAR == 2021)]\n",
    "fires2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NYPD Complaints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nypd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rename columns for easier useage\n",
    "nypd_df.rename(columns={\n",
    "    'CMPLNT_FR_DT':'COMPLAINT_DATE',\n",
    "    'BORO_NM':'BOROUGH',\n",
    "    'CMPLNT_NUM':'NUMBER_OF_COMPLAINTS'\n",
    "}, inplace=True)\n",
    "nypd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data to csv for visualization use\n",
    "nypd_df.to_csv('../data/processed/cleaned_nypd_complaint_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nypd_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COMPLAINT_DATE column to datetime\n",
    "nypd_df['COMPLAINT_DATE'] = nypd_df['COMPLAINT_DATE'].apply(lambda x: dt.datetime.strptime(x,'%m/%d/%Y %I:%M:%S %p'))\n",
    "nypd_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column that splits off the year\n",
    "nypd_df['YEAR'] = nypd_df['COMPLAINT_DATE'].dt.year\n",
    "\n",
    "# Move that column to the beginning of the frame\n",
    "year = nypd_df['YEAR']\n",
    "nypd_df.drop(labels=['YEAR'], axis=1, inplace=True)\n",
    "nypd_df.insert(0,'YEAR', year)\n",
    "nypd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training(18-19) and cross-validation(20-21) dataframes\n",
    "nypd1 = nypd_df.loc[(nypd_df.YEAR == 2018)|(nypd_df.YEAR == 2019)]\n",
    "nypd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nypd2 = nypd_df.loc[(nypd_df.YEAR == 2020) | (nypd_df.YEAR == 2021)]\n",
    "nypd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dept of Buildings/Environmental Control Board Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOB18_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOB19_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOB20_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOB21_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to designate years for each df\n",
    "DOB18_df.insert(0, 'YEAR', '2018')\n",
    "DOB19_df.insert(0, 'YEAR', '2019')\n",
    "DOB20_df.insert(0, 'YEAR', '2020')\n",
    "DOB21_df.insert(0, 'YEAR', '2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOB21_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Borough Number with names based on file schema \n",
    "# 1 = Manhattan\n",
    "# 2 = Bronx\n",
    "# 3 = Brooklyn\n",
    "# 4 = Queens\n",
    "# 5 = Staten Island\n",
    "\n",
    "def f(x):\n",
    "    if x['BORO'] == 1: return 'MANHATTAN'\n",
    "    elif x['BORO'] == 2: return 'BRONX'\n",
    "    elif x['BORO'] == 3: return 'BROOKLYN'\n",
    "    elif x['BORO'] == 4: return 'QUEENS'\n",
    "    elif x['BORO'] == 5: return 'STATEN ISLAND'\n",
    "    else: return ''\n",
    "\n",
    "DOB18_df['BOROUGH'] = DOB18_df.apply(f, axis=1)\n",
    "DOB19_df['BOROUGH'] = DOB19_df.apply(f, axis=1) \n",
    "DOB20_df['BOROUGH'] = DOB20_df.apply(f, axis=1)\n",
    "DOB21_df['BOROUGH'] = DOB21_df.apply(f, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOB18_df.head()\n",
    "# DOB19_df.head()\n",
    "# DOB20_df.head()\n",
    "DOB21_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 2018 and 2019 dataframes for training\n",
    "dobv1= DOB18_df.append(DOB19_df)\n",
    "dobv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 2020 and 2021 dataframes for cross-validation of predictive algorithm\n",
    "dobv2 = DOB20_df.append(DOB21_df)\n",
    "dobv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data for clean data file\n",
    "dobv_df = dobv1.append(dobv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data to csv for visualization use\n",
    "dobv_df.to_csv('../data/processed/cleaned_dob_violations_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for easier usage\n",
    "dobv1.rename(columns={\n",
    "    'DOB_VIOLATION_NUMBER': 'DOB/ECB_VIOLATION_COUNT'\n",
    "    }, inplace=True)\n",
    "dobv1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dobv2.rename(columns={\n",
    "    'DOB_VIOLATION_NUMBER': 'DOB/ECB_VIOLATION_COUNT'\n",
    "    }, inplace=True)\n",
    "dobv2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "dobv1 = dobv1[['YEAR',\n",
    "               'BOROUGH',\n",
    "               'ECB_VIOLATION_COUNT'\n",
    "]]\n",
    "dobv1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dobv2 = dobv2[['YEAR',\n",
    "                'BOROUGH',\n",
    "                'ECB_VIOLATION_COUNT'\n",
    "]]\n",
    "dobv2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use groupby to \"roll-up\" count data\n",
    "dobv1 = dobv1.groupby(['YEAR', 'BOROUGH']).agg({'ECB_VIOLATION_COUNT':'sum'}).reset_index()\n",
    "dobv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dobv2 = dobv2.groupby(['YEAR', 'BOROUGH']).agg({'ECB_VIOLATION_COUNT':'sum'}).reset_index()\n",
    "dobv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing Maintenance Code Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "codev_df.rename(columns={\n",
    "    'NOVIssuedDate': 'VIOLATION_ISSUE_DATE',\n",
    "    'ViolationID': 'NUMBER_OF_VIOLATIONS',\n",
    "    'Borough': 'BOROUGH'\n",
    "}, inplace=True)\n",
    "codev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data to csv for visualization use\n",
    "codev_df.to_csv('../data/processed/cleaned_housing_code_violation_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "codev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert VIOLATION_ISSUE_DATE column to datetime\n",
    "codev_df['VIOLATION_ISSUE_DATE'] = codev_df['VIOLATION_ISSUE_DATE'].apply(lambda x: dt.datetime.strptime(x,'%m/%d/%Y %I:%M:%S %p'))\n",
    "codev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add a column that splits off the year\n",
    "codev_df['YEAR'] = codev_df['VIOLATION_ISSUE_DATE'].dt.year\n",
    "\n",
    "# Move that column to the beginning of the frame\n",
    "year = codev_df['YEAR']\n",
    "codev_df.drop(labels=['YEAR'], axis=1, inplace=True)\n",
    "codev_df.insert(0,'YEAR', year)\n",
    "codev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split into training(18-19) and cross-validation(20-21) dataframes\n",
    "codev1 = codev_df.loc[(codev_df.YEAR == 2018)|(codev_df.YEAR == 2019)]\n",
    "codev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "codev2 = codev_df.loc[(codev_df.YEAR == 2020) | (codev_df.YEAR == 2021)]\n",
    "codev2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orders to Vacate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "vacate_df.rename(columns={\n",
    "    'VACATE EFFECTIVE DATE': 'VACATE_DATE',\n",
    "    'BOROUGH': 'BORO',\n",
    "    'VACATE ORDER NUMBER': 'NUMBER_OF_VACATE_ORDERS'\n",
    "}, inplace=True)\n",
    "vacate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Borough abbreviations with names based on file schema \n",
    "# MN = Manhattan\n",
    "# BX = Bronx\n",
    "# BK = Brooklyn\n",
    "# QN = Queens\n",
    "# SI = Staten Island\n",
    "\n",
    "def f(x):\n",
    "    if x['BORO'] == 'MN': return 'MANHATTAN'\n",
    "    elif x['BORO'] == 'BX': return 'BRONX'\n",
    "    elif x['BORO'] == 'BK': return 'BROOKLYN'\n",
    "    elif x['BORO'] == 'QN': return 'QUEENS'\n",
    "    elif x['BORO'] == 'SI': return 'STATEN ISLAND'\n",
    "    else: return ''\n",
    "\n",
    "vacate_df['BOROUGH'] = vacate_df.apply(f, axis=1)\n",
    "vacate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "vacate_df = vacate_df[['VACATE_DATE',\n",
    "                       'BOROUGH',\n",
    "                       'NUMBER_OF_VACATE_ORDERS']]\n",
    "vacate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned data to csv for visualization use\n",
    "vacate_df.to_csv('../data/processed/cleaned_order_to_vacate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vacate_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert VACATE_DATE column to datetime\n",
    "vacate_df['VACATE_DATE'] = vacate_df['VACATE_DATE'].apply(lambda x: dt.datetime.strptime(x,'%m/%d/%Y %I:%M:%S %p'))\n",
    "vacate_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add a column that splits off the year\n",
    "vacate_df['YEAR'] = vacate_df['VACATE_DATE'].dt.year\n",
    "\n",
    "# Move that column to the beginning of the frame\n",
    "year = vacate_df['YEAR']\n",
    "vacate_df.drop(labels=['YEAR'], axis=1, inplace=True)\n",
    "vacate_df.insert(0,'YEAR', year)\n",
    "vacate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split into training(18-19) and cross-validation(20-21) dataframes\n",
    "vacate1 = vacate_df.loc[(vacate_df.YEAR == 2018)|(vacate_df.YEAR == 2019)]\n",
    "vacate1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vacate2 = vacate_df.loc[(vacate_df.YEAR == 2020) | (vacate_df.YEAR == 2021)]\n",
    "vacate2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to load all variables into\n",
    "data1 = pd.DataFrame()\n",
    "data2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fires 1 dataset\n",
    "fires1.rename(columns={\n",
    "    'INCIDENT_DATETIME':'DATE',\n",
    "    'INCIDENT_BOROUGH':'BOROUGH',\n",
    "    'STARFIRE_INCIDENT_ID':'FIRE_COUNT'\n",
    "},inplace=True)\n",
    "fires1['BOROUGH'] = fires1['BOROUGH'].replace(['RICHMOND / STATEN ISLAND'],'STATEN ISLAND')\n",
    "fires1['YEAR'] = fires1['YEAR'].apply(str)\n",
    "fires1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fires 2 Dataset\n",
    "fires2.rename(columns={\n",
    "    'INCIDENT_DATETIME':'DATE',\n",
    "    'INCIDENT_BOROUGH':'BOROUGH',\n",
    "    'STARFIRE_INCIDENT_ID':'FIRE_COUNT'\n",
    "},inplace=True)\n",
    "fires2['BOROUGH'] = fires2['BOROUGH'].replace(['RICHMOND / STATEN ISLAND'],'STATEN ISLAND')\n",
    "fires2['YEAR'] = fires2['YEAR'].apply(str)\n",
    "fires2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DOB/ECB Violations 1 dataset\n",
    "# dobv1.rename(columns={\n",
    "#     'YEAR_OF_COMPLAINT':'YEAR',\n",
    "#     'ECB_VIOLATION_COUNT':'DOB/ECB_VIOLATION_COUNT'\n",
    "# },inplace=True)\n",
    "# dobv1['DATE'] = dobv1['YEAR'].map(lambda x: '2018-01-01' if '2018'in x else '2019-01-01' if '2019' else '' )\n",
    "# dobv1['DATE'] = dobv1['DATE'].apply(lambda x: dt.datetime.strptime(x,'%Y-%m-%d'))\n",
    "# dobv1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DOB/ECB Violations 2 dataset\n",
    "# dobv2.rename(columns={\n",
    "#     'YEAR_OF_COMPLAINT':'YEAR',\n",
    "#     'ECB_VIOLATION_COUNT':'DOB/ECB_VIOLATION_COUNT'\n",
    "# },inplace=True)\n",
    "# dobv2['DATE'] = dobv2['YEAR'].map(lambda x: '2020-01-01' if '2020'in x else '2021-01-01' if '2021' else '' )\n",
    "# dobv2['DATE'] = dobv2['DATE'].apply(lambda x: dt.datetime.strptime(x,'%Y-%m-%d'))\n",
    "# dobv2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housing Code Violations 1 dataset\n",
    "codev1.rename(columns={\n",
    "    'VIOLATION_ISSUE_DATE':'DATE',\n",
    "    'NUMBER_OF_VIOLATIONS':'HOUSING_CODE_VIOLATION_COUNT'\n",
    "},inplace=True)\n",
    "codev1['YEAR'] = codev1['YEAR'].apply(str)\n",
    "codev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housing Code Violations 2 dataset\n",
    "codev2.rename(columns={\n",
    "    'VIOLATION_ISSUE_DATE':'DATE',\n",
    "    'NUMBER_OF_VIOLATIONS':'HOUSING_CODE_VIOLATION_COUNT'\n",
    "},inplace=True)\n",
    "codev2['YEAR'] = codev2['YEAR'].apply(str)\n",
    "codev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYPD Complaints 1 dataset\n",
    "nypd1.rename(columns={\n",
    "    'COMPLAINT_DATE':'DATE',\n",
    "    'NUMBER_OF_COMPLAINTS':'NYPD_COMPLAINT_COUNT',\n",
    "},inplace=True)\n",
    "nypd1['YEAR'] = nypd1['YEAR'].apply(str)\n",
    "nypd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYPD Complaints 2 dataset\n",
    "nypd2.rename(columns={\n",
    "    'COMPLAINT_DATE':'DATE',\n",
    "    'NUMBER_OF_COMPLAINTS':'NYPD_COMPLAINT_COUNT',\n",
    "},inplace=True)\n",
    "nypd2['YEAR'] = nypd2['YEAR'].apply(str)\n",
    "nypd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders to Vacate 1 Dataset\n",
    "vacate1.rename(columns={\n",
    "    'VACATE_DATE': 'DATE',\n",
    "    'NUMBER_OF_VACATE_ORDERS':'VACATE_ORDER_COUNT'\n",
    "},inplace=True)\n",
    "vacate1['YEAR'] = vacate1['YEAR'].apply(str)\n",
    "vacate1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders to Vacate 2 Dataset\n",
    "vacate2.rename(columns={\n",
    "    'VACATE_DATE': 'DATE',\n",
    "    'NUMBER_OF_VACATE_ORDERS':'VACATE_ORDER_COUNT'\n",
    "},inplace=True)\n",
    "vacate2['YEAR'] = vacate2['YEAR'].apply(str)\n",
    "vacate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge all dataframes \n",
    "data1 = pd.merge(fires1, dobv1, on=['YEAR','DATE', 'BOROUGH'], how='outer')\n",
    "# data1 = pd.merge(data1, nypd1, on=['YEAR','DATE', 'BOROUGH'], how='outer')\n",
    "# data1 = pd.merge(data1, codev1, on=['YEAR','DATE', 'BOROUGH'], how='outer')\n",
    "# data1 = pd.merge(data1, vacate1, on=['YEAR','DATE', 'BOROUGH'], how='outer')\n",
    "data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv('../data/processed/summary_counts_2018_2019.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes \n",
    "data2 = pd.merge(fires2, dobv2, on=['YEAR','DATE', 'BOROUGH'], how='outer')\n",
    "data2 = pd.merge(data2, nypd2, on=['YEAR','DATE', 'BOROUGH'], how='outer')\n",
    "data2 = pd.merge(data2, codev2, on=['YEAR','DATE', 'BOROUGH'], how='outer')\n",
    "data2 = pd.merge(data2, vacate2, on=['YEAR','DATE', 'BOROUGH'], how='outer')\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('../data/processed/summary_counts_2020_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
